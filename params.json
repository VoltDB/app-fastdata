{"name":"App-fastdata","tagline":"","body":"# VoltDB Fast Data Example App\r\n\r\nUse Case\r\n--------\r\n\r\nThe Fast Data app simulates real-time click stream processing. Click events are\r\ningested into VoltDB at a high rate, then cleaned up and persisted into a data\r\nwarehouse for historical analysis. Segmentation information is calculated in\r\ndata warehouse and stored back into VoltDB. VoltDB uses the information to\r\nsegment real-time events for per-event decisioning.\r\n\r\nThe click events can come from persistent queues like [Apache\r\nKafka](http://kafka.apache.org). For simplicity, this sample app generates\r\nrandom events at a constant rate. Each click event contains basic information\r\nlike source IP address, destination URL, timestamp, HTTP method name, referral\r\nURL, and the user agent string. More information can be included in a real-world\r\nuse case.\r\n\r\nThe stream is ingested into VoltDB, cleaned up, then exported into a data\r\nwarehouse like [Hadoop](http://hadoop.apache.org) for long term persistence. The\r\ndata warehouse runs machine learning algorithm on the full historical dataset to\r\nsegment the click events into clusters periodically. Each cluster is represented\r\nby its center. The cluster centers are sent back into VoltDB.\r\n\r\nVoltDB uses the clustering information to further segment new click events into\r\nthe corresponding cluster at real-time. VoltDB can use this to make per-event\r\nreal-time decisions like what advertisement to show a user, or whether a user\r\nshould be blocked for spamming.\r\n\r\nEvents are aged out of VoltDB after they are persisted in the data\r\nwarehouse. This limits the dataset inside VoltDB to only include relatively hot\r\ndata.\r\n\r\nThe example includes a dashboard that shows click stream cluster distribution,\r\ntop users and top visited URLs in a moving window. All of these information is\r\ncalculated from materialized views on relevant source tables.\r\n\r\nCode organization\r\n-----------------\r\n\r\nThe code is divided into projects:\r\n\r\n- \"db\": the database project, which contains the schema, stored procedures and\r\n  other configurations that are compiled into a catalog and run in a VoltDB\r\n  database.\r\n- \"client\": a java client that generates click events.\r\n- \"web\": a simple web server that provides the demo dashboard.\r\n- \"hadoop\": a collection of pig/shell scripts, and a Spark program\r\n\r\nSee below for instructions on running these applications.  For any questions,\r\nplease contact fieldengineering@voltdb.com.\r\n\r\nPre-requisites\r\n--------------\r\n\r\n1. Before running these scripts you need to have VoltDB 4.8 or later installed,\r\n   and the bin subdirectory should be added to your PATH environment variable.\r\n   For example, if you installed VoltDB Enterprise 4.8 in your home directory,\r\n   you could add it to the PATH with the following command:\r\n\r\n    ```bash\r\n    export PATH=\"$PATH:$HOME/voltdb-ent-4.8/bin\"\r\n    ```\r\n    \r\nHadoop Demo\r\n-----------\r\n\r\nThis demo requires a [Cloudera](http://www.cloudera.com/content/cloudera/en/downloads/cloudera_manager/cm-5-2-1.html)\r\nHadoop installation, whose configured services also include [Spark](https://spark.apache.org/).\r\nThe demo consists of a VoltDB server writing export data to files stored in Hadoop, and\r\na set of scripts, and programs that collect the exported data, compute k-means clusters on the\r\ncollected data, and store the computations back to VoltDB.\r\n\r\n1. On the server where VoltDB is installed set the environment variable\r\n   `WEBHDFS_ENDPOINT` to a WebHDFS endpoint that matches the following pattern\r\n\r\n   ```\r\n   http://[host]:[port]/webhdfs/v1/[export-base-directory]/%g/%p/%t.avro?user.name=[user]\r\n   ```\r\n2. Download this [archive](http://downloads.voltdb.com/technologies/other/fastdata-kmeans.tar.bz2)\r\n   and unpack it on an Hadoop node\r\n\r\n   ```bash\r\n   $ tar -jxf fastdata-kmeans.tar.bz2\r\n   ```\r\n3. Change your working directory to `fastdata-kmeans` and run the `compute_clusters.sh`\r\n   script when you to want to process exported data from VoltDB (see \r\n   [**Demo Instructions**](#demo-instructions) section bellow)\r\n\r\n   ```bash\r\n   $ cd fastdata-kmeans\r\n   #\r\n   # Assuming you are exporting to export/fastdata, and VoltDB is\r\n   # running on volthost\r\n   $ ./compute_clusters.sh export/fastdata volthost\r\n   ```\r\n\r\n   This script\r\n\r\n   * Harvests the exported data by renaming the export base directory\r\n   * Invokes the `harvest.pig` pig script to write the harvested data into a\r\n     Parquet database\r\n   * Starts a Spark job that computes the K-Means clusters on the data stored\r\n     in the parquet database, and creates another parquet db with the resulting\r\n     computations.\r\n   * Invokes the `load.pig` script that reads the K-Means computation Parquet\r\n     database, and writes its content back to VoltDB by utilizing the\r\n     [VoltDB hadoop extensions](https://github.com/VoltDB/voltdb-hadoop-extension)\r\n\r\nVertica Demo\r\n------------\r\n\r\n1. This example also requires [Vertica](http://www.vertica.com) installed on the\r\n   same machine or a machine that the VoltDB machine has access to. A Vertica\r\n   database must be created with the username **dbadmin** with no password. Once\r\n   Vertica is running, set the environment variable `VERTICAIP` to point to the\r\n   Vertica machine on the VoltDB machine. For example, if your Vertica is\r\n   running on 192.168.0.1, run the following command on the VoltDB machine:\r\n    ```bash\r\n    export VERTICAIP=\"192.168.0.1\"\r\n    ```\r\n\r\n1. VoltDB uses JDBC to export click events to Vertica. Vertica's JDBC driver\r\n   must be present in the classpath before starting VoltDB. If your Vertica is\r\n   installed in `/opt/vertica`, you can find the JDBC driver in\r\n   `/opt/vertica/java/lib/`. Copy the JAR file in that directory to the VoltDB\r\n   machine and put it in the `lib/extension` sub-directory in your VoltDB\r\n   installation.\r\n\r\n1. To run the K-means clustering algorithm in Vertica, it requires the R\r\n   language package to be installed. Please follow the instructions in [Vertica\r\n   documentation](https://my.vertica.com/docs/7.1.x/HTML/index.htm#Authoring/ProgrammersGuide/UserDefinedFunctions/UDxR/InstallingRForHPVertica.htm)\r\n   to install the R language pack.\r\n\r\n1. Copy the `vertica` sub-directory in the example to the vertica machine. This\r\n   directory contains Vertica extensions for K-means clustering and loading data\r\n   back into VoltDB. The following instructions assume that the directory is\r\n   copied to `/tmp/vertica`.\r\n\r\nDemo Instructions\r\n-----------------\r\n\r\n1. Start the web server\r\n    ```bash\r\n    ./run.sh start_web\r\n    ```\r\n\r\n2. Start the database and client\r\n    ```bash\r\n    # for Hadoop run\r\n    $ ./run.sh hadoop_demo\r\n    # while for Vertical run\r\n    $ ./run.sh demo\r\n    ```\r\n\r\n3. Open a web browser to http://hostname:8081\r\n\r\n4. For Vertica run the `updatemodel.sh` script on the Vertica machine to run\r\n   the K-means clustering algorithm on the data in Vertica. You can run this\r\n   command periodically to update the cluster model in VoltDB.\r\n    ```bash\r\n    $ /tmp/vertica/updatemodel.sh\r\n    ```\r\n\r\n5. To stop the demo:\r\n\r\nStop the client (if it hasn't already completed) by pressing Ctrl-C\r\n\r\nStop the database\r\n```bash\r\n$ voltadmin shutdown\r\n```\r\n\r\nStop the web server\r\n```bash\r\n$ ./run.sh stop_web\r\n```\r\n\r\nOptions\r\n-------\r\n\r\nYou can control various characteristics of the demo by modifying the parameters\r\npassed into the LogGenerator java application in the \"client\" function within\r\nthe run.sh script.\r\n\r\nSpeed & Duration:\r\n\r\n    --duration=120                (benchmark duration in seconds)\r\n    --ratelimit=20000             (run up to this rate of requests/second)\r\n\r\n\r\nInstructions for running on a cluster\r\n-------------------------------------\r\n\r\nBefore running this demo on a cluster, make the following changes:\r\n\r\n1. On each server, edit the run.sh file to set the HOST variable to the name of\r\n   the **first** server in the cluster:\r\n\r\n    HOST=voltserver01\r\n\r\n2. On each server, edit db/deployment.xml to change hostcount from 1 to the\r\n   actual number of servers:\r\n    ```\r\n    <cluster hostcount=\"1\" sitesperhost=\"3\" kfactor=\"0\" />\r\n    ```\r\n\r\n4. On each server, start the database\r\n    ```bash\r\n    ./run.sh server\r\n    ```\r\n\r\n5. On one server, Edit the run.sh script to set the SERVERS variable to a\r\n   comma-separated list of the servers in the cluster\r\n\r\n    SERVERS=voltserver01,voltserver02,voltserver03\r\n\r\n6. Run the client script:\r\n    ```bash\r\n    ./run.sh client\r\n    ```\r\n","google":"UA-130533-10","note":"Don't delete this file! It's used internally to help with page regeneration."}